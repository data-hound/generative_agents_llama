{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3gm/generative_agents_llama/blob/main/Generative_Agents_with_Llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng4xDrNglwDM"
      },
      "source": [
        "# Generative Agents with Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPVlsGmrhRr8"
      },
      "source": [
        "<img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Isabella_Rodriguez.png\" alt=\"Generative Isabella\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Sam_Moore.png\" alt=\"Generative Sam\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Eddy_Lin.png\" alt=\"Generative Eddy\">\n",
        " <img src=\"https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Wolfgang_Schulz.png\" alt=\"Generative Wolfgang\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfF0DnaOg-wV"
      },
      "source": [
        "The goal is to use open-source models in agent simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XzuihslyUt"
      },
      "source": [
        "## Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXPCppW6LIRs"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/R3gm/generative_agents_llama.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM7IHaUeV7cO"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4yaIxcmzq6fu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available on this system.\n",
            "Using pip 23.3.1 from /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/pip (python 3.9)\n",
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies: started\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command pip subprocess to install build dependencies\n",
            "  Collecting setuptools>=42\n",
            "    Downloading setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Collecting scikit-build>=0.13\n",
            "    Downloading scikit_build-0.17.6-py3-none-any.whl.metadata (14 kB)\n",
            "  Collecting cmake>=3.18\n",
            "    Downloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "  Collecting ninja\n",
            "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "  Collecting distro (from scikit-build>=0.13)\n",
            "    Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Collecting packaging (from scikit-build>=0.13)\n",
            "    Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Collecting tomli (from scikit-build>=0.13)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
            "    Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
            "  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/863.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/863.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.4/863.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "  \u001b[?25hDownloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
            "  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "  \u001b[?25hDownloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/26.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/26.7 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/26.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/26.7 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/26.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/26.7 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/26.7 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m21.5/26.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m24.6/26.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "  \u001b[?25hUsing cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "  Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
            "  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "  \u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
            "  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "  \u001b[?25hDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n",
            "  \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  langchain-core 0.1.13 requires packaging<24.0,>=23.2, but you have packaging 24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "  \u001b[0mSuccessfully installed cmake-3.29.3 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-70.0.0 tomli-2.0.1 wheel-0.43.0\n",
            "  Running command Getting requirements to build wheel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info\n",
            "  writing /tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-hei2luii/llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-hei2luii/llama_cpp_python-0.1.78.dist-info'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
            "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/e1/4d/d612de852a0bc64a64418e1cef25fe1914c5b1611e34cc271ed7e36174c8/typing_extensions-4.12.0-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
            "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/54/30/c2a907b9443cf42b90c17ad10c1e8fa801975f01cb9764f3f8eb8aea638b/numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m155.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "\n",
            "\n",
            "  --------------------------------------------------------------------------------\n",
            "  -- Trying 'Ninja' generator\n",
            "  --------------------------------\n",
            "  ---------------------------\n",
            "  ----------------------\n",
            "  -----------------\n",
            "  ------------\n",
            "  -------\n",
            "  --\n",
            "  \u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "    CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "  \u001b[0mNot searching for unused variables given on the command line.\n",
            "\n",
            "  -- The C compiler identification is GNU 9.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- The CXX compiler identification is GNU 9.4.0\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Configuring done (0.7s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_cmake_test_compile/build\n",
            "  --\n",
            "  -------\n",
            "  ------------\n",
            "  -----------------\n",
            "  ----------------------\n",
            "  ---------------------------\n",
            "  --------------------------------\n",
            "  -- Trying 'Ninja' generator - success\n",
            "  --------------------------------------------------------------------------------\n",
            "\n",
            "  Configuring Project\n",
            "    Working directory:\n",
            "      /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-build\n",
            "    Command:\n",
            "      /tmp/pip-build-env-0d0sh785/overlay/lib/python3.9/site-packages/cmake/data/bin/cmake /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-0d0sh785/overlay/lib/python3.9/site-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install -DPYTHON_VERSION_STRING:STRING=3.9.18 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-0d0sh785/overlay/lib/python3.9/site-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/home/as1296/miniconda3/envs/llama_GA/bin/python -DPYTHON_INCLUDE_DIR:PATH=/home/as1296/miniconda3/envs/llama_GA/include/python3.9 -DPYTHON_LIBRARY:PATH=/home/as1296/miniconda3/envs/llama_GA/lib/libpython3.9.so -DPython_EXECUTABLE:PATH=/home/as1296/miniconda3/envs/llama_GA/bin/python -DPython_ROOT_DIR:PATH=/home/as1296/miniconda3/envs/llama_GA -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/home/as1296/miniconda3/envs/llama_GA/include/python3.9 -DPython3_EXECUTABLE:PATH=/home/as1296/miniconda3/envs/llama_GA/bin/python -DPython3_ROOT_DIR:PATH=/home/as1296/miniconda3/envs/llama_GA -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/home/as1296/miniconda3/envs/llama_GA/include/python3.9 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-0d0sh785/overlay/lib/python3.9/site-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
            "\n",
            "  Not searching for unused variables given on the command line.\n",
            "  -- The C compiler identification is GNU 9.4.0\n",
            "  -- The CXX compiler identification is GNU 9.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.25.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "  \u001b[0m\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "  -- Check if compiler accepts -pthread\n",
            "  -- Check if compiler accepts -pthread - yes\n",
            "  -- Found Threads: TRUE\n",
            "  -- Could not find nvcc, please set CUDAToolkit_ROOT.\n",
            "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:291 (message):\n",
            "    cuBLAS not found\n",
            "\n",
            "  \u001b[0m\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Configuring done (0.6s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-build\n",
            "  [1/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  [2/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
            "  [3/8] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  [4/8] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  [5/8] Linking C static library vendor/llama.cpp/libggml_static.a\n",
            "  [6/8] Linking C shared library vendor/llama.cpp/libggml_shared.so\n",
            "  [7/8] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
            "  [7/8] Install the project...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install/lib/libllama.so\n",
            "  -- Installing: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install/bin/convert.py\n",
            "  -- Installing: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/_skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/libllama.so\n",
            "\n",
            "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py\n",
            "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py\n",
            "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py\n",
            "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py\n",
            "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py\n",
            "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py\n",
            "  creating directory _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server\n",
            "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py\n",
            "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py\n",
            "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py\n",
            "  copying /tmp/pip-install-2x_gxsd3/llama-cpp-python_edc2994937b14625b84b3ba6aee4def8/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/py.typed\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
            "  copied 9 files\n",
            "  running build_ext\n",
            "  installing to _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
            "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
            "  copied 11 files\n",
            "  running install_data\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  copying _skbuild/linux-x86_64-3.9/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing llama_cpp_python.egg-info/PKG-INFO\n",
            "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
            "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
            "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
            "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE.md'\n",
            "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
            "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.9.egg-info\n",
            "  running install_scripts\n",
            "  copied 0 files\n",
            "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-jbwxq8uo/.tmp-ypo5mjq2/llama_cpp_python-0.1.78-cp39-cp39-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'llama_cpp/__init__.py'\n",
            "  adding 'llama_cpp/libllama.so'\n",
            "  adding 'llama_cpp/llama.py'\n",
            "  adding 'llama_cpp/llama_cpp.py'\n",
            "  adding 'llama_cpp/llama_grammar.py'\n",
            "  adding 'llama_cpp/llama_types.py'\n",
            "  adding 'llama_cpp/py.typed'\n",
            "  adding 'llama_cpp/utils.py'\n",
            "  adding 'llama_cpp/server/__init__.py'\n",
            "  adding 'llama_cpp/server/__main__.py'\n",
            "  adding 'llama_cpp/server/app.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
            "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
            "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
            "  removing _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp39-cp39-linux_x86_64.whl size=731325 sha256=bf00b0f338fad90cc58382bfd9d51c76935cc26e7f105791308948b81f5853e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3txh8m9n/wheels/0c/b7/c5/44bc04b1032871ad214d9e23594bbffeccff9278f98691d8b3\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/__pycache__/typing_extensions.cpython-39.pyc\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/typing_extensions-4.9.0.dist-info/\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/bin/f2py\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/bin/f2py3\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/bin/f2py3.9\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/numpy-1.25.2.dist-info/\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/numpy.libs/\n",
            "      Removing file or directory /home/as1296/miniconda3/envs/llama_GA/lib/python3.9/site-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  changing mode of /home/as1296/miniconda3/envs/llama_GA/bin/f2py to 775\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.26.4 typing-extensions-4.12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.1.13 requires packaging<24.0,>=23.2, but you have packaging 23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %cd generative_agents_llama\n",
        "# !python -m pip install -r requirements.txt\n",
        "\n",
        "import torch\n",
        "import os\n",
        "try:\n",
        "  from llama_cpp import Llama\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "      print(\"CUDA is available on this system.\")\n",
        "      os.system('CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose')\n",
        "  else:\n",
        "      print(\"CUDA is not available on this system.\")\n",
        "      os.system('pip install llama-cpp-python==0.1.78')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acNb3r_9WGEb"
      },
      "source": [
        "## Configure the model\n",
        "\n",
        "You can change the following parameters to choose the model you require.\n",
        "\n",
        "```\n",
        "repo_model = \"TheBloke/OpenOrca-Platypus2-13B-GGML\"\n",
        "filename_model = \"openorca-platypus2-13b.ggmlv3.q4_1.bin\"\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QXj-nvsOmI1j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting reverie/backend_server/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile reverie/backend_server/utils.py\n",
        "# Select a model\n",
        "repo_model = \"TheBloke/OpenOrca-Platypus2-13B-GGML\"\n",
        "filename_model = \"openorca-platypus2-13b.ggmlv3.q4_1.bin\"\n",
        "\n",
        "# repo_model = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\"\n",
        "# filename_model = \"Meta-Llama-3-8B-Instruct.Q4_1.gguf\"\n",
        "\n",
        "temperature_model = 0.5\n",
        "\n",
        "# Put your name\n",
        "key_owner = \"Generic_User\"\n",
        "\n",
        "maze_assets_loc = \"../../environment/frontend_server/static_dirs/assets\"\n",
        "env_matrix = f\"{maze_assets_loc}/the_ville/matrix\"\n",
        "env_visuals = f\"{maze_assets_loc}/the_ville/visuals\"\n",
        "\n",
        "fs_storage = \"../../environment/frontend_server/storage\"\n",
        "fs_temp_storage = \"../../environment/frontend_server/temp_storage\"\n",
        "\n",
        "collision_block_id = \"32125\"\n",
        "\n",
        "# Verbose\n",
        "debug = True\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWBpvEMFWyMC"
      },
      "source": [
        "## Start the frontend server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTxEzbudylkq"
      },
      "outputs": [],
      "source": [
        "%cd /content/generative_agents_llama/environment/frontend_server\n",
        "\n",
        "!nohup python manage.py runserver 127.0.0.1:8000 &\n",
        "!echo \"$(<nohup.out )\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI0r9fGDdwn7"
      },
      "source": [
        "The server is running in http://localhost:8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yywN9mJ0W4ii"
      },
      "source": [
        "### For Colab\n",
        " If you are running this notebook in Colab, you need to create an ngrok tunnel to access the frontend.\n",
        " Get it in https://dashboard.ngrok.com/get-started/your-authtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW-IkfyVDzVA"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "authtoken_ngrok = \"\" # @param {type:\"string\"}\n",
        "!ngrok config add-authtoken {authtoken_ngrok}\n",
        "\n",
        "http_tunnel = ngrok.connect(\"http://localhost:8000\")\n",
        "http_tunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1nYbNpYdplg"
      },
      "source": [
        "Open the NgrokTunnel for see the frontend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_MtZ7WdVBQ"
      },
      "source": [
        "## Start the backend_server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPIpd8Ru_d6x"
      },
      "outputs": [],
      "source": [
        "%cd /content/generative_agents_llama/reverie/backend_server/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcBvR8Gbd-EL"
      },
      "source": [
        "After the model is loaded:\n",
        "\n",
        "1. Enter name of forked simulation: `base_the_ville_isabella_maria_klaus`\n",
        "2. Enter name of new simulation: `simulation1`\n",
        "3. Enter the number of steps you want to run the simulation: `run 10` to run 10 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz0mqS6HJkp_"
      },
      "outputs": [],
      "source": [
        "!python reverie.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiiG2nPye5dG"
      },
      "source": [
        "- For see the simulation Go to: `http://localhost:8000/simulator_home` or your `<NgrokTunnel>/simulator_home`\n",
        "- To replay, `http://localhost:8000/replay/simulation1/1/` or your `<NgrokTunnel>/replay/simulation1/1/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OxNej6mgKgY"
      },
      "source": [
        "### Notes\n",
        "- Issues with generating motion in the frontend still unresolved.\n",
        "- The model might not generate proper responses, which could lead to errors."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
